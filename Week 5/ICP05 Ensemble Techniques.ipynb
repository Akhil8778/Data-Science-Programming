{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxGJbLsUhuc8"
   },
   "source": [
    "# Week05 -\n",
    "\n",
    "In this week we look at using ensembles of models to improve the performance of our models. We will look at the following:\n",
    "\n",
    "* RandomForest\n",
    "* AdaBoost\n",
    "* Gradiant Boosting\n",
    "* XG Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tuXRZKEYrDa"
   },
   "source": [
    "## Introduction and Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q08EVUytY3eh"
   },
   "source": [
    "In this notebook, we will reuse the Universal Bank dataset.\n",
    "\n",
    "This time, we are developing a model to predict whether a customer will accept a personal loan offer. The dataset contains 5000 observations and 14 variables. The data is available on one of my GitHub repos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmYLcm3aY8X5"
   },
   "source": [
    "## Install and import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.4-py3-none-win_amd64.whl (89.1 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\akhil\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.4\n"
     ]
    }
   ],
   "source": [
    "# You may need to install xgboost (it's not part of the sklearn package)\n",
    "# !conda install xgboost \n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "8zNdljvIhuc8"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGgrXNQPZT3J"
   },
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "q3u5LsGyhudA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  Mortgage  \\\n",
       "0   1   25           1      49     91107       4    1.6          1         0   \n",
       "1   2   45          19      34     90089       3    1.5          1         0   \n",
       "2   3   39          15      11     94720       1    1.0          1         0   \n",
       "3   4   35           9     100     94112       1    2.7          2         0   \n",
       "4   5   35           8      45     91330       4    1.0          2         0   \n",
       "\n",
       "   Personal Loan  Securities Account  CD Account  Online  CreditCard  \n",
       "0              0                   1           0       0           0  \n",
       "1              0                   1           0       0           0  \n",
       "2              0                   0           0       0           0  \n",
       "3              0                   0           0       0           0  \n",
       "4              0                   0           0       0           1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./UniversalBank.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aOH_GFGZZFx"
   },
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "OkUM_mnHhudC",
    "outputId": "b4e542fe-5d03-4602-e4d9-06a6d0e7c65d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  Mortgage  \\\n",
      "0   1   25           1      49     91107       4    1.6          1         0   \n",
      "1   2   45          19      34     90089       3    1.5          1         0   \n",
      "2   3   39          15      11     94720       1    1.0          1         0   \n",
      "3   4   35           9     100     94112       1    2.7          2         0   \n",
      "4   5   35           8      45     91330       4    1.0          2         0   \n",
      "\n",
      "   Personal Loan  Securities Account  CD Account  Online  CreditCard  \n",
      "0              0                   1           0       0           0  \n",
      "1              0                   1           0       0           0  \n",
      "2              0                   0           0       0           0  \n",
      "3              0                   0           0       0           0  \n",
      "4              0                   0           0       0           1  \n",
      "Index(['ID', 'Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg',\n",
      "       'Education', 'Mortgage', 'Personal Loan', 'Securities Account',\n",
      "       'CD Account', 'Online', 'CreditCard'],\n",
      "      dtype='object')\n",
      "                ID          Age   Experience       Income      ZIP Code  \\\n",
      "count  5000.000000  5000.000000  5000.000000  5000.000000   5000.000000   \n",
      "mean   2500.500000    45.338400    20.104600    73.774200  93152.503000   \n",
      "std    1443.520003    11.463166    11.467954    46.033729   2121.852197   \n",
      "min       1.000000    23.000000    -3.000000     8.000000   9307.000000   \n",
      "25%    1250.750000    35.000000    10.000000    39.000000  91911.000000   \n",
      "50%    2500.500000    45.000000    20.000000    64.000000  93437.000000   \n",
      "75%    3750.250000    55.000000    30.000000    98.000000  94608.000000   \n",
      "max    5000.000000    67.000000    43.000000   224.000000  96651.000000   \n",
      "\n",
      "            Family        CCAvg    Education     Mortgage  Personal Loan  \\\n",
      "count  5000.000000  5000.000000  5000.000000  5000.000000    5000.000000   \n",
      "mean      2.396400     1.937938     1.881000    56.498800       0.096000   \n",
      "std       1.147663     1.747659     0.839869   101.713802       0.294621   \n",
      "min       1.000000     0.000000     1.000000     0.000000       0.000000   \n",
      "25%       1.000000     0.700000     1.000000     0.000000       0.000000   \n",
      "50%       2.000000     1.500000     2.000000     0.000000       0.000000   \n",
      "75%       3.000000     2.500000     3.000000   101.000000       0.000000   \n",
      "max       4.000000    10.000000     3.000000   635.000000       1.000000   \n",
      "\n",
      "       Securities Account  CD Account       Online   CreditCard  \n",
      "count         5000.000000  5000.00000  5000.000000  5000.000000  \n",
      "mean             0.104400     0.06040     0.596800     0.294000  \n",
      "std              0.305809     0.23825     0.490589     0.455637  \n",
      "min              0.000000     0.00000     0.000000     0.000000  \n",
      "25%              0.000000     0.00000     0.000000     0.000000  \n",
      "50%              0.000000     0.00000     1.000000     0.000000  \n",
      "75%              0.000000     0.00000     1.000000     1.000000  \n",
      "max              1.000000     1.00000     1.000000     1.000000  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ID                  5000 non-null   int64  \n",
      " 1   Age                 5000 non-null   int64  \n",
      " 2   Experience          5000 non-null   int64  \n",
      " 3   Income              5000 non-null   int64  \n",
      " 4   ZIP Code            5000 non-null   int64  \n",
      " 5   Family              5000 non-null   int64  \n",
      " 6   CCAvg               5000 non-null   float64\n",
      " 7   Education           5000 non-null   int64  \n",
      " 8   Mortgage            5000 non-null   int64  \n",
      " 9   Personal Loan       5000 non-null   int64  \n",
      " 10  Securities Account  5000 non-null   int64  \n",
      " 11  CD Account          5000 non-null   int64  \n",
      " 12  Online              5000 non-null   int64  \n",
      " 13  CreditCard          5000 non-null   int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 547.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "# read the first row of the dataset \n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiaaNFX2Zf-I"
   },
   "source": [
    "## Clean/transform data (where necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "3JuJlVGDkINJ",
    "outputId": "082781c3-db3c-44e8-a7fd-ba35f35cbbfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg',\n",
       "       'Education', 'Mortgage', 'Personal Loan', 'Securities Account',\n",
       "       'CD Account', 'Online', 'CreditCard'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on findings from data exploration, we need to clean up colum names, as there are some leading whitespace characters\n",
    "df.columns = [s.strip() for s in df.columns] \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns we are not using as predictors (see previous notebooks -- we are given a subset of input variables to consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['ID', 'ZIP Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "      <th>Edu_2</th>\n",
       "      <th>Edu_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Experience  Income  Family  CCAvg  Mortgage  Personal Loan  \\\n",
       "0   25           1      49       4    1.6         0              0   \n",
       "1   45          19      34       3    1.5         0              0   \n",
       "2   39          15      11       1    1.0         0              0   \n",
       "\n",
       "   Securities Account  CD Account  Online  CreditCard  Edu_2  Edu_3  \n",
       "0                   1           0       0           0      0      0  \n",
       "1                   1           0       0           0      0      0  \n",
       "2                   0           0       0           0      0      0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translation education categories into dummy vars\n",
    "df = df.join(pd.get_dummies(df['Education'], prefix='Edu', drop_first=True))\n",
    "df.drop('Education', axis=1, inplace = True)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKY30W1pZxCP"
   },
   "source": [
    "## Split data intro training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "d0fAfB0ThudG",
    "outputId": "47f231af-3781-4603-95b8-d9a1fca0236e"
   },
   "outputs": [],
   "source": [
    "# construct datasets for analysis\n",
    "target = 'Personal Loan'\n",
    "predictors = list(df.columns)\n",
    "predictors.remove(target)\n",
    "X = df[predictors]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "t0DkCAoChudI",
    "outputId": "4f5824b6-d5e0-419c-c916-6be218916af2"
   },
   "outputs": [],
   "source": [
    "# create the training set and the test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2A_u7rQhuc_"
   },
   "source": [
    "## Prediction with Decision Tree (using default parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30gNRdX-qtNT"
   },
   "source": [
    "You can find details about SKLearm's DecisionTree classifier [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbPfUmcEXf6K"
   },
   "source": [
    "Create a decision tree using all of the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UZ60Vn1AhudK"
   },
   "outputs": [],
   "source": [
    "dtree=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntgxBhvJXkjp"
   },
   "source": [
    "Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "xPL4rRlVhudM",
    "outputId": "db26a1f0-23c9-4a02-87c5-34e3dbd71ccf"
   },
   "outputs": [],
   "source": [
    "_ = dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMvD4-9wXy_1"
   },
   "source": [
    "Review of the performance of the model on the validation/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "YAcO31dIX7JE",
    "outputId": "797a7c84-5c1c-4ec8-c75e-12c675ea8061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.9060402684563759\n",
      "Accuracy Score:   0.9873333333333333\n",
      "Precision Score:  0.9642857142857143\n",
      "F1 Score:         0.9342560553633219\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recall result from this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with RandomForest (using default parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all our classifiers, RandomeForestClassifier has a number of parameters that can be adjusted/tuned. In this example below, we simply accept the defaults. You may want to experiment with changing the defaul values and also use GridSearchCV to explore ranges of values.\n",
    "\n",
    "* n_estimators: The number of trees in the forsest\n",
    "    - A deeper tree might increase the performance, but also the complexity and chances to overfit.\n",
    "    - The value must be an integer greater than 0. Default is 100.  \n",
    "* max_depth: The maximum depth per tree. \n",
    "    - Deeper trees might increase the performance, but also the complexity and chances to overfit.\n",
    "    - The value must be an integer greater than 0. Default is None, which allows the tree to grow without constraint.\n",
    "* See the SciKit Learn documentation for more details. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = rforest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.8456375838926175\n",
      "Accuracy Score:   0.9833333333333333\n",
      "Precision Score:  0.984375\n",
      "F1 Score:         0.9097472924187726\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recall result from this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "500 fits failed out of a total of 5000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "265 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "235 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.88516508 0.87910448 0.90619629 0.88213478 0.8819991  0.87001357\n",
      " 0.86399819 0.88810493 0.86698327 0.86399819 0.616237   0.88516508\n",
      " 0.87001357 0.88516508 0.89719584 0.90325645 0.88208955 0.88511986\n",
      " 0.87607417 0.89416554 0.85793758 0.88516508 0.68285844 0.86092266\n",
      " 0.80361827 0.89113523 0.79145183 0.90321122 0.89118046 0.87905925\n",
      "        nan 0.86096789 0.89421076 0.91230213 0.85486205 0.89118046\n",
      " 0.85481682 0.88815016 0.8761194  0.80352782 0.89412031 0.88208955\n",
      " 0.91524197 0.87598372        nan 0.79760289 0.86707372 0.77630032\n",
      " 0.86399819 0.85798281 0.88208955 0.88819539 0.87304387 0.8700588\n",
      " 0.59529625 0.8761194  0.89412031        nan 0.86395296 0.89719584\n",
      " 0.89118046 0.85192221        nan 0.87905925 0.88213478 0.87914971\n",
      " 0.36856626        nan 0.78846676 0.87602895        nan 0.90018091\n",
      " 0.86996834 0.90018091 0.87901402 0.89416554 0.85490728        nan\n",
      " 0.87602895 0.88208955 0.86399819 0.8700588  0.87910448 0.86698327\n",
      " 0.89412031 0.88208955 0.51044776 0.87001357 0.87607417 0.87607417\n",
      " 0.87910448 0.86702849 0.86698327 0.89421076 0.87602895 0.86996834\n",
      " 0.87905925 0.88810493 0.89715061        nan 0.88208955 0.89719584\n",
      " 0.88511986 0.86101312 0.88516508 0.87304387 0.89113523 0.85187698\n",
      " 0.88516508 0.83677069 0.89719584 0.66779738 0.89719584 0.86399819\n",
      " 0.87607417 0.88815016        nan 0.88507463 0.84278607 0.91221167\n",
      " 0.87304387 0.3412483  0.86996834 0.87001357 0.87905925 0.86096789\n",
      "        nan 0.88819539 0.86399819 0.87607417 0.86395296 0.89715061\n",
      " 0.89719584 0.88507463 0.8700588  0.75237449 0.88815016 0.87299864\n",
      " 0.88815016 0.8761194         nan 0.72514699 0.89118046        nan\n",
      " 0.8700588  0.87910448 0.90022614 0.89416554 0.74608774 0.81270918\n",
      " 0.62555405 0.85187698 0.87001357 0.82763455 0.82451379 0.86702849\n",
      " 0.78539123 0.87299864 0.90321122 0.87901402 0.87901402 0.87001357\n",
      " 0.86702849 0.90321122 0.83993668 0.88819539 0.88810493 0.86404342\n",
      " 0.86399819 0.8700588  0.89719584 0.89719584        nan        nan\n",
      " 0.86092266 0.90022614 0.88810493 0.86101312 0.86702849 0.85798281\n",
      " 0.87001357 0.87905925 0.86399819 0.8700588  0.88516508 0.8761194\n",
      "        nan 0.87914971 0.87299864        nan        nan 0.8700588\n",
      "        nan 0.87607417 0.73419267 0.87602895 0.87905925 0.89109\n",
      " 0.89421076        nan 0.88815016 0.89421076 0.86702849 0.87014925\n",
      " 0.88208955 0.88511986 0.86399819        nan 0.87910448 0.85789236\n",
      " 0.87602895 0.88213478        nan 0.8700588  0.90018091 0.88511986\n",
      " 0.90321122 0.87910448 0.90022614 0.88204432 0.86399819        nan\n",
      " 0.88213478 0.75540479        nan 0.83387607 0.87304387 0.87910448\n",
      " 0.89118046 0.86698327 0.86092266        nan 0.89118046 0.88208955\n",
      " 0.87304387 0.86101312 0.86101312        nan 0.88810493 0.88208955\n",
      " 0.90321122 0.87304387        nan 0.86395296 0.89719584 0.87905925\n",
      " 0.88815016 0.85789236 0.87905925 0.85798281        nan        nan\n",
      " 0.89719584        nan 0.87905925 0.85793758        nan 0.89118046\n",
      " 0.89416554 0.89122569 0.88218001 0.86702849 0.87001357 0.88810493\n",
      " 0.85490728 0.90325645 0.86399819        nan 0.87602895 0.62550882\n",
      " 0.89416554 0.87607417 0.86698327 0.88213478 0.62234283 0.33817277\n",
      " 0.86698327        nan 0.88516508 0.87602895 0.89421076 0.88204432\n",
      " 0.88810493 0.89416554 0.87001357 0.70085934 0.87607417 0.89109\n",
      " 0.89724107 0.86092266 0.74016282 0.89113523 0.85789236 0.89724107\n",
      " 0.88810493 0.90321122 0.90022614 0.88815016 0.88511986 0.87001357\n",
      " 0.83061963 0.8488919  0.86707372 0.89113523 0.90624152 0.88516508\n",
      " 0.88208955 0.89113523 0.87607417 0.89113523 0.58602442 0.86698327\n",
      " 0.86702849 0.86395296 0.86698327 0.89412031 0.90018091 0.87001357\n",
      " 0.89113523        nan 0.88204432 0.88516508 0.86698327 0.79764812\n",
      " 0.89715061 0.89416554 0.89719584 0.87299864 0.89113523 0.8730891\n",
      " 0.88208955        nan        nan 0.87001357 0.89122569 0.86702849\n",
      " 0.86702849 0.85793758 0.88516508 0.73093623 0.88815016 0.90624152\n",
      " 0.90018091 0.87905925 0.87304387 0.32912709 0.88511986 0.90624152\n",
      " 0.86096789 0.88208955 0.87001357 0.79461782 0.89710538 0.83071009\n",
      " 0.88815016        nan 0.89421076        nan 0.87001357 0.86395296\n",
      " 0.89416554 0.87607417 0.88819539 0.88511986 0.87304387 0.88815016\n",
      " 0.86702849 0.88819539 0.85183175 0.89113523 0.89122569 0.86395296\n",
      " 0.87299864 0.88507463 0.87910448 0.84278607 0.88516508 0.86399819\n",
      " 0.87607417 0.88810493 0.88815016 0.88208955 0.87607417 0.88507463\n",
      " 0.82161918 0.87905925 0.87304387 0.88511986        nan 0.90018091\n",
      " 0.88511986 0.90619629 0.90619629 0.90022614 0.86698327 0.89113523\n",
      " 0.90922659 0.89421076        nan 0.85798281 0.88208955 0.87001357\n",
      "        nan 0.87607417 0.87905925 0.85789236 0.61935776 0.89416554\n",
      " 0.87910448 0.8730891  0.8761194  0.85793758 0.90316599 0.88511986\n",
      " 0.87001357        nan 0.89416554 0.88213478 0.85789236 0.87001357\n",
      " 0.82157395 0.89416554 0.70999548        nan 0.89113523 0.87607417\n",
      " 0.6012664  0.83975577 0.89421076 0.83686115        nan        nan\n",
      " 0.87001357 0.88511986 0.89113523 0.8761194  0.87905925 0.86395296\n",
      " 0.90321122 0.86101312 0.89421076 0.7914066  0.88213478 0.90619629\n",
      "        nan        nan 0.84278607 0.89416554 0.86698327 0.87910448\n",
      " 0.86096789 0.67064677 0.72216192 0.81858887 0.56802352        nan\n",
      " 0.87001357        nan 0.87602895 0.87304387        nan 0.87607417\n",
      " 0.87304387 0.89416554 0.89113523 0.88213478 0.89421076 0.89118046\n",
      " 0.90624152 0.87001357 0.88810493        nan 0.89412031 0.84274084\n",
      " 0.87910448 0.89421076 0.89113523 0.87607417 0.89113523        nan\n",
      " 0.84902759        nan 0.87304387 0.87304387 0.90321122 0.90018091\n",
      " 0.86092266 0.90018091 0.86395296 0.89724107 0.86101312 0.89416554\n",
      " 0.90321122 0.90624152        nan        nan 0.90018091 0.60438716\n",
      " 0.90018091 0.86096789 0.88213478 0.88824062 0.86092266 0.90022614\n",
      " 0.88819539 0.88213478        nan 0.87602895 0.87602895 0.89719584\n",
      "        nan 0.86092266 0.90321122 0.89715061 0.87598372 0.87607417\n",
      " 0.88516508 0.87299864 0.89719584 0.90022614 0.88511986 0.61334238\n",
      " 0.75223881 0.88815016 0.86092266 0.89421076 0.86395296 0.8428313\n",
      " 0.88208955 0.89715061 0.87607417 0.87299864 0.88218001 0.89118046\n",
      " 0.63464496 0.87910448 0.88815016 0.86395296 0.87910448 0.90018091\n",
      "        nan 0.88507463 0.89715061 0.89118046 0.76422433 0.57087291\n",
      " 0.44676617        nan 0.72808684 0.8700588  0.89719584 0.88819539\n",
      " 0.8761194  0.89113523        nan 0.88815016 0.90624152 0.88511986\n",
      "        nan        nan 0.87001357        nan 0.86698327 0.87299864\n",
      " 0.87304387 0.88815016 0.88511986 0.86096789 0.89421076 0.83369516\n",
      " 0.87607417 0.88815016 0.88516508 0.7492085  0.89113523 0.90316599\n",
      " 0.86702849 0.88208955 0.86096789 0.87905925 0.41071913 0.89113523\n",
      " 0.86698327 0.89416554 0.8700588  0.87607417 0.85789236 0.8700588\n",
      " 0.83677069 0.90922659 0.56499322 0.86395296        nan 0.78860244\n",
      " 0.8066938  0.79439168 0.86996834 0.90018091 0.88815016 0.90018091\n",
      " 0.88213478        nan 0.88511986 0.88511986 0.8700588  0.86996834\n",
      " 0.87598372 0.89113523 0.87905925 0.87607417 0.85793758        nan\n",
      " 0.60719132 0.84581637 0.86702849 0.87905925 0.90918137 0.88516508\n",
      " 0.88815016 0.7431479  0.33505201 0.81886024 0.90316599 0.89416554\n",
      " 0.89416554 0.90316599 0.86096789 0.89412031 0.86996834 0.54961556\n",
      " 0.48326549 0.37155133 0.89421076 0.88511986 0.87910448 0.88815016\n",
      " 0.87905925 0.88511986 0.89122569 0.72202623 0.87313433 0.88218001\n",
      " 0.8458616  0.89724107 0.88516508        nan 0.89416554 0.87905925\n",
      " 0.88819539 0.83378562 0.78236092 0.89416554 0.90633198 0.89719584\n",
      "        nan 0.88507463 0.89113523 0.89416554 0.89416554 0.90013569\n",
      " 0.8700588  0.89719584 0.79145183        nan 0.88507463 0.83075531\n",
      " 0.90018091        nan 0.65884215 0.37458164 0.86702849 0.70402533\n",
      " 0.89118046 0.7883763         nan 0.87910448 0.90316599 0.87299864\n",
      " 0.75531434 0.8761194  0.86702849 0.71582994 0.88213478 0.86707372\n",
      " 0.87602895 0.85481682 0.88815016 0.89416554 0.89715061 0.89113523\n",
      "        nan 0.72211669 0.89715061 0.8488919  0.89113523 0.90624152\n",
      " 0.89416554 0.89118046 0.89113523 0.87304387 0.87001357 0.8700588\n",
      " 0.89113523 0.86096789 0.87001357 0.89416554 0.87914971 0.84880145\n",
      " 0.51967436 0.8761194  0.89719584 0.89416554 0.92130258 0.89118046\n",
      " 0.87910448 0.87602895 0.90022614 0.86698327 0.90022614 0.66472185\n",
      " 0.86399819 0.87607417 0.89416554 0.89715061 0.90321122        nan\n",
      " 0.88511986 0.87304387 0.86399819 0.88204432 0.87001357 0.87001357\n",
      " 0.89109    0.87001357 0.87607417 0.86390773 0.89421076 0.85187698\n",
      " 0.90316599 0.88511986 0.89113523 0.89118046 0.88815016 0.87304387\n",
      " 0.90316599 0.89113523 0.63147897 0.90316599 0.88507463 0.84590683\n",
      " 0.89710538 0.89421076 0.80054274 0.77327001 0.89710538 0.89715061\n",
      " 0.87910448        nan 0.78249661 0.89118046        nan 0.85192221\n",
      " 0.88815016 0.87905925 0.85798281 0.89719584        nan 0.87607417\n",
      " 0.90918137 0.86698327 0.89113523 0.90619629 0.89719584        nan\n",
      " 0.87607417 0.87607417 0.87304387 0.70995025 0.78258706 0.85784713\n",
      " 0.86698327 0.86702849 0.87616463 0.87304387 0.87910448 0.90624152\n",
      " 0.87910448 0.87001357        nan 0.90918137 0.90316599 0.87607417\n",
      " 0.89118046 0.80967888        nan 0.62546359 0.86404342 0.87905925\n",
      "        nan 0.86693804 0.90018091 0.82763455 0.88511986 0.67987336\n",
      "        nan 0.88511986 0.88516508 0.83690638 0.88511986 0.87001357\n",
      " 0.90018091 0.84581637 0.87602895 0.87304387 0.41374943 0.87304387\n",
      " 0.87607417 0.87304387        nan 0.89421076 0.90022614 0.90018091\n",
      " 0.89719584 0.87001357 0.88819539 0.86399819        nan 0.87905925\n",
      " 0.88815016 0.89719584 0.73722298 0.71013116 0.86395296 0.88810493\n",
      " 0.81876979 0.87607417 0.83374039 0.86702849 0.67670737 0.88516508\n",
      " 0.89719584 0.8700588  0.88208955 0.79479873        nan 0.82469471\n",
      " 0.88815016 0.839801   0.84884668 0.87607417        nan 0.89719584\n",
      " 0.85789236 0.87607417        nan 0.88810493 0.87304387 0.67078245\n",
      " 0.88507463 0.88819539 0.89113523 0.87910448        nan 0.86395296\n",
      " 0.86698327 0.89118046 0.89416554 0.89719584 0.87602895 0.89715061\n",
      " 0.87901402 0.88810493 0.90022614 0.90624152 0.88815016 0.87304387\n",
      " 0.88511986 0.88511986 0.89118046 0.8458616  0.89715061 0.90316599\n",
      " 0.90624152 0.32912709 0.88810493 0.87607417 0.87607417 0.89113523\n",
      " 0.75214835 0.89719584 0.8880597  0.87910448 0.89113523 0.89118046\n",
      "        nan 0.89421076 0.89421076 0.90321122 0.90022614        nan\n",
      " 0.78552691 0.8428313  0.46218905 0.90619629 0.89118046 0.72795115\n",
      "        nan 0.88819539 0.85490728        nan 0.88815016 0.45314337\n",
      " 0.87910448 0.89715061 0.85192221 0.87905925 0.79452736 0.82473994\n",
      " 0.88208955 0.88810493 0.87304387 0.58611488 0.89719584 0.89416554\n",
      " 0.88213478 0.86693804 0.90018091 0.89416554        nan        nan\n",
      " 0.88208955 0.86698327 0.77354138 0.78530077 0.86702849 0.87010403\n",
      " 0.87602895 0.88204432 0.88810493 0.83677069 0.4804161  0.88204432\n",
      " 0.88819539        nan        nan 0.89407508 0.87602895 0.81854365\n",
      " 0.89113523 0.88511986 0.65251018 0.89109    0.90018091 0.89113523\n",
      " 0.90321122 0.91221167 0.89122569 0.89416554        nan 0.89416554\n",
      " 0.90018091 0.87001357 0.88810493 0.86689281        nan 0.88507463\n",
      " 0.89118046 0.87299864 0.88213478 0.88511986        nan 0.90018091\n",
      "        nan 0.83989145 0.87602895 0.74902759        nan        nan\n",
      " 0.90325645 0.87598372 0.74328358 0.62831298 0.88511986 0.85793758\n",
      " 0.87001357 0.81248304 0.9152872  0.74319313 0.8730891  0.83071009\n",
      " 0.88521031 0.85495251 0.90321122 0.88208955 0.86096789 0.89113523\n",
      " 0.86698327 0.87299864 0.37141565 0.88815016]\n",
      "  warnings.warn(\n",
      "C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.95392796 0.93956547 1.         0.99849057 0.99924528 0.93881361\n",
      " 0.92823328 0.95090337 0.93881075 0.92823328 0.68430532 0.95317038\n",
      " 0.94863065 0.94562035 0.99546884 0.99093768 0.9365466  0.9441052\n",
      " 0.92295026 0.96601487 0.92143796 0.95467696 0.73866495 0.92597198\n",
      " 0.84665237 0.96752144 0.85950257 0.99849057 0.96298742 0.94409949\n",
      "        nan 0.92522013 0.99093482 0.99924528 0.93125786 0.95619211\n",
      " 0.92823899 0.95694111 0.9539251  0.86329331 0.9539251  0.95316467\n",
      " 1.         0.95618639        nan 0.85721269 0.92898513 0.84895655\n",
      " 0.93578616 0.92445397 0.96072041 0.95769583 0.93352487 0.92823899\n",
      " 0.63744425 0.93578616 0.96449971        nan 0.93503431 0.96525443\n",
      " 0.94938822 0.92596055        nan 0.94788451 0.94939108 0.92672098\n",
      " 0.40103774        nan 0.85044311 0.95165809        nan 0.98338193\n",
      " 0.93504288 0.99471412 0.93805889 0.94561464 0.91389365        nan\n",
      " 0.94107776 0.95316752 0.93579188 0.93654088 0.93730417 0.94335049\n",
      " 1.         0.93956832 0.55740995 0.93956832 0.93503145 0.93125786\n",
      " 0.93503145 0.92672384 0.93200972 0.98867067 0.93654088 0.93579474\n",
      " 0.93881361 0.94334763 0.97129217        nan 0.96374214 0.97204403\n",
      " 0.9403259  0.92067467 0.94561178 0.93125214 0.95770154 0.90935963\n",
      " 0.95241567 0.88746427 0.99849057 0.6902916  0.95543168 0.92370212\n",
      " 0.93428531 0.94259291        nan 0.9622327  0.92294454 1.\n",
      " 0.93578902 0.35497427 0.93654374 0.9403259  0.97582905 0.93805889\n",
      "        nan 0.95317038 0.93050314 0.94334477 0.92521727 0.95467696\n",
      " 0.9501458  0.95618925 0.92899657 0.79905946 0.95996569 0.9501458\n",
      " 0.95619211 0.94108062        nan 0.75979417 0.95467696        nan\n",
      " 0.9259777  0.94787879 0.99471412 0.96071755 0.78926529 0.87989708\n",
      " 0.65931961 0.92143511 0.93730417 0.89123499 0.8965323  0.92974557\n",
      " 0.86556032 0.93654374 0.9622327  0.93730132 0.95014866 0.92748428\n",
      " 0.95543168 1.         0.8783562  0.95241281 0.96449686 0.95618353\n",
      " 0.93126358 0.92446541 0.96902516 0.95769868        nan        nan\n",
      " 0.92824185 0.99622356 0.95316752 0.92596913 0.92521441 0.92672384\n",
      " 0.9614837  0.96072899 0.92748428 0.92672098 0.93956832 0.93427673\n",
      "        nan 0.93578902 0.95392224        nan        nan 0.93880789\n",
      "        nan 0.9403259  0.79077187 0.95921384 0.94183819 0.9486335\n",
      " 0.99244425        nan 0.95392224 0.95392224 0.92370212 0.92746998\n",
      " 0.9720526  0.9501458  0.92899371        nan 0.94561464 0.93503716\n",
      " 0.98262722 0.931255          nan 0.92521441 0.95921098 0.92824471\n",
      " 1.         1.         0.99924528 0.99848771 0.91992281        nan\n",
      " 0.9327673  0.8285163         nan 0.89649514 0.98035735 0.93277015\n",
      " 0.95618639 0.9327673  0.92219554        nan 0.95769868 0.94561464\n",
      " 0.93881647 0.93276158 0.92370783        nan 0.94788165 0.95165237\n",
      " 0.99773299 0.94182962        nan 0.92823899 0.96902516 0.9531761\n",
      " 0.95014866 0.91917095 0.95090051 0.93201258        nan        nan\n",
      " 0.99471412        nan 0.96072613 0.92370783        nan 0.95392224\n",
      " 0.96524871 0.94409949 0.94108348 0.93050029 0.9403259  0.95240995\n",
      " 0.92445683 0.97204403 0.92596913        nan 0.93503145 0.6835163\n",
      " 0.95317038 0.93956547 0.93048313 0.99773585 0.66993425 0.35422813\n",
      " 0.93427673        nan 0.94863636 0.94939108 0.95543453 0.95391652\n",
      " 0.95241281 0.95543739 0.93654374 0.7794454  0.95467696 1.\n",
      " 0.99848771 0.93504288 0.79908519 0.95240709 0.93049743 0.9901801\n",
      " 0.95543739 1.         0.97054317 0.95996569 0.94712407 0.92748142\n",
      " 0.89577759 0.92596055 0.93277015 0.97507433 0.99471412 0.94712407\n",
      " 0.95618639 0.95316752 0.93730132 1.         0.6125243  0.93578616\n",
      " 0.92672384 0.92899943 0.93427673 0.97129217 0.99924242 0.92899371\n",
      " 0.95316752        nan 0.95845626 0.95090337 0.9372956  0.86779302\n",
      " 0.96903087 0.96902802 0.99395941 0.93956832 0.97356204 0.92672956\n",
      " 0.99773013        nan        nan 0.94561464 0.97204974 0.93805317\n",
      " 0.93201258 0.91161521 0.94712979 0.79455689 0.9463665  0.9916924\n",
      " 0.96072041 0.93351915 0.93352773 0.34062607 0.95619211 0.99924242\n",
      " 0.92672384 0.96072041 0.9441052  0.86250429 0.97507433 0.90180389\n",
      " 0.95694969        nan 0.96524871        nan 0.95090051 0.92974557\n",
      " 0.95166095 0.93578616 0.95921098 0.9365466  0.9267267  0.95468268\n",
      " 0.92748142 0.95467982 0.92823899 0.95317038 0.95619211 0.92068611\n",
      " 0.9577044  0.95391938 0.93427673 0.90483133 0.95467696 0.93730132\n",
      " 0.93427959 0.95090051 0.95543453 0.95619497 0.95921098 0.95240995\n",
      " 0.90482847 0.93805889 0.93427959 0.99395083        nan 0.97129217\n",
      " 0.94939394 0.98942824 0.99093482 0.95996569 0.93730132 0.96751572\n",
      " 1.         0.97507433        nan 0.92522013 0.94712121 0.95014866\n",
      "        nan 0.94259577 0.93653802 0.92748142 0.66918239 0.95468268\n",
      " 0.93352487 0.92446255 0.92672098 0.92823613 0.99018296 0.94258719\n",
      " 0.93881361        nan 0.96525729 0.9539251  0.92748714 0.93579474\n",
      " 0.87234706 0.95543739 0.75678674        nan 0.97129503 0.92974271\n",
      " 0.64799886 0.9048199  0.97582905 0.9229474         nan        nan\n",
      " 0.92445969 0.97356204 0.94712693 0.94183248 0.94485134 0.91615209\n",
      " 0.97960263 0.92143511 0.95240709 0.85875929 0.95695254 1.\n",
      "        nan        nan 0.90708977 0.95845626 0.92899657 0.95996855\n",
      " 0.92370783 0.71600343 0.77865638 0.88142367 0.61626072        nan\n",
      " 0.94335049        nan 0.93578616 0.93805317        nan 0.94032876\n",
      " 0.94108919 0.95241281 1.         0.93805889 0.97280446 0.94938822\n",
      " 1.         0.92370497 0.95165237        nan 0.98488851 0.90935963\n",
      " 0.93806175 0.97280732 0.96525443 0.93880789 0.95618639        nan\n",
      " 0.90255575        nan 0.92823613 0.9372956  0.9962207  0.96600629\n",
      " 0.92748142 0.99849057 0.93427959 1.         0.92672384 0.95543453\n",
      " 1.         0.99924242        nan        nan 0.97507433 0.65026015\n",
      " 0.97130074 0.92446255 0.95317038 0.95618639 0.92823613 0.95543168\n",
      " 0.95996284 0.98035735        nan 0.93956261 0.94108062 0.99924528\n",
      "        nan 0.92748428 0.99773299 0.99546884 0.93125786 0.95316752\n",
      " 0.97054317 0.96600915 0.95391938 1.         0.95392224 0.65405089\n",
      " 0.83081189 0.97054031 0.92521441 0.95090051 0.93805317 0.8987793\n",
      " 0.93579188 0.95165523 0.96676958 0.96223842 0.95996569 0.95316752\n",
      " 0.68048313 0.9365466  0.95618925 0.92068325 0.94485706 0.98791595\n",
      "        nan 0.94259291 0.95845912 0.9501458  0.82024014 0.60272727\n",
      " 0.47737278        nan 0.78096055 0.92899085 0.97281018 0.95392796\n",
      " 0.93881933 0.98715838        nan 0.9539251  1.         0.94938822\n",
      "        nan        nan 0.93200972        nan 0.91916524 0.95694397\n",
      " 0.95316467 0.95694397 0.95921098 0.92823613 0.96903373 0.90483419\n",
      " 0.94939108 0.95618639 0.95392224 0.80058605 0.97356489 1.\n",
      " 0.92747856 0.9622327  0.93805317 0.99698113 0.43806747 0.95996569\n",
      " 0.9320183  0.95996569 0.9267267  0.92973699 0.92899085 0.95240995\n",
      " 0.88745283 1.         0.59816181 0.93050886        nan 0.84665809\n",
      " 0.87383362 0.86026015 0.93730417 1.         0.95392796 0.9864008\n",
      " 0.94560892        nan 0.96072041 0.94032304 0.92974843 0.94108348\n",
      " 0.96902516 0.95618639 0.97281018 0.94183248 0.9267267         nan\n",
      " 0.64050029 0.91313036 0.93729846 0.97356204 1.         0.95241281\n",
      " 0.95543739 0.8270526  0.35722985 0.8942024  0.99018296 0.95769868\n",
      " 0.99697827 1.         0.9267267  0.96751858 0.93126072 0.61409091\n",
      " 0.51212979 0.39499142 0.96751858 0.9508948  0.94863636 0.95845054\n",
      " 0.93730989 0.95467982 0.96072899 0.79228988 0.92219268 0.93881361\n",
      " 0.91463979 0.97734134 0.94863636        nan 0.97280446 0.94183819\n",
      " 0.95467982 0.88746427 0.83229274 0.96978273 1.         0.97658662\n",
      "        nan 0.95543453 0.95316752 0.95845912 0.95392224 1.\n",
      " 0.92521727 0.97356204 0.85272441        nan 0.95845626 0.88293025\n",
      " 0.98035735        nan 0.73333905 0.41541166 0.93654088 0.76961407\n",
      " 0.96374786 0.84062893        nan 0.93956261 1.         0.93503716\n",
      " 0.8164494  0.93805603 0.931255   0.75756432 0.95014294 0.93201258\n",
      " 0.95619497 0.92446255 0.95316752 0.97205832 0.98715266 0.9372956\n",
      "        nan 0.76508862 1.         0.91236993 0.9539251  0.99546598\n",
      " 0.9765809  0.97356775 0.96222985 0.92899085 0.93730417 0.93427673\n",
      " 0.95392224 0.92521441 0.93125214 0.98640366 0.94032018 0.92899085\n",
      " 0.55283305 0.93730417 0.98338193 0.95392224 1.         0.94712407\n",
      " 0.95769868 0.94863065 0.97053459 0.94334763 0.97280446 0.74092624\n",
      " 0.92748142 0.94259005 0.9486335  1.         0.97129503        nan\n",
      " 0.95769868 0.93805889 0.93277301 0.98110921 0.92445683 0.92899943\n",
      " 0.99924528 0.94032018 0.95015152 0.93729846 0.964494   0.91388508\n",
      " 0.98338193 0.94561178 0.96978559 0.95166095 0.95845626 0.94258719\n",
      " 0.9962207  0.94937965 0.67749571 0.9864008  0.94863636 0.92445969\n",
      " 1.         0.97204688 0.87235277 0.8285506  0.97658376 0.97129503\n",
      " 0.94183248        nan 0.87081475 0.95543453        nan 0.91841624\n",
      " 0.95391652 0.97733562 0.92143511 0.98413951        nan 0.92445683\n",
      " 1.         0.93503716 0.95543453 1.         0.97582905        nan\n",
      " 0.98942253 0.94032018 0.93805031 0.75679245 0.85342481 0.92144368\n",
      " 0.92445969 0.93050029 0.92370783 0.92748142 0.95391652 1.\n",
      " 0.93806175 0.93352201        nan 0.99244711 0.99924528 0.94183819\n",
      " 0.96222985 0.8859434         nan 0.67973413 0.91842196 0.93805603\n",
      "        nan 0.93503716 0.97506575 0.8912207  0.95165809 0.72732704\n",
      "        nan 0.95694683 0.94032018 0.90633505 0.94259291 0.92974557\n",
      " 0.98866781 0.91917095 0.96072613 0.93352487 0.43654374 0.92748142\n",
      " 0.93201544 0.95165809        nan 0.97582333 0.96147513 1.\n",
      " 0.95165809 0.93427959 0.94184105 0.92370497        nan 0.93654946\n",
      " 0.94182962 0.99924528 0.79076615 0.76656375 0.92823613 0.97356489\n",
      " 0.8776072  0.93427101 0.90557747 0.92748142 0.69638651 0.95770154\n",
      " 0.96978273 0.9372956  0.98262722 0.8760749         nan 0.90784734\n",
      " 0.93956832 0.89576615 0.90406804 0.9403259         nan 0.96072041\n",
      " 0.92446255 0.91993425        nan 0.95769868 0.92824185 0.70767296\n",
      " 0.96450257 0.97053745 0.95391938 0.95543453        nan 0.93729846\n",
      " 0.92899371 0.95090051 0.95240995 0.95996855 0.93880789 1.\n",
      " 0.93200972 0.95316467 0.99395941 1.         0.95618925 0.93881361\n",
      " 0.9546741  0.97431961 0.95845912 0.92219268 1.         0.98942539\n",
      " 1.         0.36251001 0.95317038 0.94108062 0.93578902 0.95392224\n",
      " 0.8338279  0.98036021 0.94787879 0.94788165 0.96525157 0.95996569\n",
      "        nan 0.99622642 0.97431389 0.98338193 0.99093482        nan\n",
      " 0.85192396 0.91313036 0.47352773 1.         0.9486335  0.81344197\n",
      "        nan 0.98489137 0.92974843        nan 0.96600915 0.50980274\n",
      " 0.96827902 0.9539251  0.92521155 0.93956832 0.86629788 0.88291309\n",
      " 0.94259291 0.95090337 0.93806175 0.63142939 0.97129503 0.95543739\n",
      " 0.94183533 0.93200686 0.98035735 0.95694683        nan        nan\n",
      " 0.95089766 0.95090051 0.83531447 0.84668668 0.92672384 0.93881075\n",
      " 0.93201258 0.95166095 0.9584534  0.895749   0.54153516 0.94787879\n",
      " 0.9539251         nan        nan 0.95845054 0.9463665  0.88745855\n",
      " 0.95392796 0.95391938 0.70242424 0.97054031 0.97431961 0.98564894\n",
      " 0.98263007 1.         0.96676387 0.97205546        nan 0.96525157\n",
      " 0.99395083 0.95996284 0.95316467 0.94787879        nan 0.96071755\n",
      " 0.95618925 0.9410749  0.94863636 0.9539251         nan 0.99849057\n",
      "        nan 0.91539165 0.95619497 0.80062607        nan        nan\n",
      " 1.         0.93352201 0.79982561 0.67520583 0.95769868 0.93202115\n",
      " 0.95845626 0.91841338 1.         0.80435963 0.93503145 0.90483133\n",
      " 0.96525157 0.92672956 1.         0.93503145 0.93200686 0.96827044\n",
      " 0.92748142 0.94334763 0.40863065 0.94939108]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9213025780189958\n",
      "... with parameters: {'min_samples_split': 3, 'min_samples_leaf': 1, 'max_leaf_nodes': 177, 'max_features': 'auto', 'max_depth': 60, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': [1,2,3,4,5,6,7,8,9,10],  \n",
    "    'min_samples_leaf': [1, 2,3, 4],\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_features' : ['auto','sqrt'],\n",
    "    'max_depth': [10,20,30,40,50,60,70,80,90,100,None],\n",
    "    'bootstrap' : [True , False]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = rf, param_distributions=param_grid, cv=kfolds, n_iter=1000,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_=rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9846667 Precision=0.9701493 Recall=0.8724832 F1=0.9187279\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomsearchcv_on_randomforest = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with ADABoost (using default parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all our classifiers, ADABoostClassifier has a number of parameters that can be adjusted/tuned. In this example below, we simply accept the defaults. You may want to experiment with changing the defaul values and also use GridSearchCV to explore ranges of values.\n",
    "\n",
    "* max_depth: The maximum depth per tree. \n",
    "    - A deeper tree might increase the performance, but also the complexity and chances to overfit.\n",
    "    - The value must be an integer greater than 0. Default is None (meaning, the tree can grow to a point where all leaves have 1 observation).\n",
    "* learning_rate: The learning rate determines the step size at each iteration while your model optimizes toward its objective. \n",
    "    - A low learning rate makes computation slower, and requires more rounds to achieve the same reduction in residual error as a model with a high learning rate. But it optimizes the chances to reach the best optimum.\n",
    "    - Larger learning rates may not converge on a solution.\n",
    "    - The value must be between 0 and 1. Default is 0.3.\n",
    "* n_estimators: The number of trees in our ensemble. \n",
    "    - Equivalent to the number of boosting rounds.\n",
    "    - The value must be an integer greater than 0. Default is 100.\n",
    "* See the SciKit Learn documentation for more details. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "aboost = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = aboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = aboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.7248322147651006\n",
      "Accuracy Score:   0.9626666666666667\n",
      "Precision Score:  0.8780487804878049\n",
      "F1 Score:         0.7941176470588235\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recall result from this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "aboost_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all our classifiers, GradientBoostingClassifier has a number of parameters that can be adjusted/tuned. In this example below, we simply accept the defaults. You may want to experiment with changing the defaul values and also use GridSearchCV to explore ranges of values.\n",
    "\n",
    "* max_depth: The maximum depth per tree. \n",
    "    - A deeper tree might increase the performance, but also the complexity and chances to overfit.\n",
    "    - The value must be an integer greater than 0. Default is None (meaning, the tree can grow to a point where all leaves have 1 observation).\n",
    "* learning_rate: The learning rate determines the step size at each iteration while your model optimizes toward its objective. \n",
    "    - A low learning rate makes computation slower, and requires more rounds to achieve the same reduction in residual error as a model with a high learning rate. But it optimizes the chances to reach the best optimum.\n",
    "    - Larger learning rates may not converge on a solution.\n",
    "    - The value must be between 0 and 1. Default is 0.3.\n",
    "* n_estimators: The number of trees in our ensemble. \n",
    "    - Equivalent to the number of boosting rounds.\n",
    "    - The value must be an integer greater than 0. Default is 100.\n",
    "* See the SciKit Learn documentation for more details. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.8657718120805369\n",
      "Accuracy Score:   0.9826666666666667\n",
      "Precision Score:  0.9555555555555556\n",
      "F1 Score:         0.9084507042253522\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recall result from this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all our classifiers, XGBoost has a number of parameters that can be adjusted/tuned. In this example below, we simply accept the defaults. You may want to experiment with changing the defaul values and also use GridSearchCV to explore ranges of values.\n",
    "\n",
    "* max_depth: The maximum depth per tree. \n",
    "    - A deeper tree might increase the performance, but also the complexity and chances to overfit.\n",
    "    - The value must be an integer greater than 0. Default is 6.\n",
    "* learning_rate: The learning rate determines the step size at each iteration while your model optimizes toward its objective. \n",
    "    - A low learning rate makes computation slower, and requires more rounds to achieve the same reduction in residual error as a model with a high learning rate. But it optimizes the chances to reach the best optimum.\n",
    "    - The value must be between 0 and 1. Default is 0.3.\n",
    "* n_estimators: The number of trees in our ensemble. \n",
    "    - Equivalent to the number of boosting rounds.\n",
    "    - The value must be an integer greater than 0. Default is 100.\n",
    "* colsample_bytree: Represents the fraction of columns to be randomly sampled for each tree. \n",
    "    - It might improve overfitting.\n",
    "    - The value must be between 0 and 1. Default is 1.\n",
    "* subsample: Represents the fraction of observations to be sampled for each tree. \n",
    "    - A lower values prevent overfitting but might lead to under-fitting.\n",
    "    - The value must be between 0 and 1. Default is 1.\n",
    "* See the XGBoost documentation for more details. https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.8926174496644296\n",
      "Accuracy Score:   0.9853333333333333\n",
      "Precision Score:  0.9568345323741008\n",
      "F1 Score:         0.9236111111111113\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recall result from this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summarize results    \n",
    "\n",
    "As usual -- in this section you provide a recap your approach, results, and discussion of findings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall scores...\n",
      "Decision Tree:    0.9060402684563759\n",
      "Random Forest:    0.8456375838926175\n",
      "Ada Boosted Tree: 0.7248322147651006\n",
      "Gradient Tree:    0.8657718120805369\n",
      "XGBoost Tree:     0.8926174496644296\n",
      "randomsearchcv_on_randomforest:0.8926174496644296\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall scores...\")\n",
    "print(f\"{'Decision Tree:':18}{dtree_recall}\")\n",
    "print(f\"{'Random Forest:':18}{rforest_recall}\")\n",
    "print(f\"{'Ada Boosted Tree:':18}{aboost_recall}\")\n",
    "print(f\"{'Gradient Tree:':18}{gboost_recall}\")\n",
    "print(f\"{'XGBoost Tree:':18}{xgboost}\")\n",
    "print(f\"{'randomsearchcv_on_randomforest:':18}{randomsearchcv_on_randomforest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discussion section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As we can see Decision Tree has best recall amoung others. For the random forest using randomsearchcv has second best of 89.26%, It might give the best recall for random forest if we use grind search along with randomsearchcv."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Class08b-decision_tree_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b056086e24cb5602cbcb82122035cd3d6ee2ccbf5df29c16e348c108b0f83be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
