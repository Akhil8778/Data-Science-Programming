{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fcd8054",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c35905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afacfe7a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f62533",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./airbnb_train_X_price_gte_150.csv') \n",
    "y_train = pd.read_csv('./airbnb_train_y_price_gte_150.csv') \n",
    "X_test = pd.read_csv('./airbnb_test_X_price_gte_150.csv') \n",
    "y_test = pd.read_csv('./airbnb_test_y_price_gte_150.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af610372",
   "metadata": {},
   "source": [
    "##  Model the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c03864",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a515b",
   "metadata": {},
   "source": [
    "## SVM classification model using polynomial kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1264788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5b7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly_model = SVC(kernel=\"poly\", degree=3, coef0=1, C=10)\n",
    "_ = svm_poly_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "898b43eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = svm_poly_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"svm with polynomial kernel\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad347ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.867854</td>\n",
       "      <td>0.855839</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.869323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  Accuracy  Precision    Recall        F1\n",
       "0  svm with polynomial kernel  0.867854   0.855839  0.883239  0.869323"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.sort_values(by=['Precision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282f067",
   "metadata": {},
   "source": [
    "## DTree Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f130ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conduct an initial random search across a wide range of possible parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea4a80",
   "metadata": {},
   "source": [
    "# Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84bb4e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 0.8543925220428145\n",
      "... with parameters: {'min_samples_split': 24, 'min_samples_leaf': 11, 'min_impurity_decrease': 0.0031, 'max_leaf_nodes': 56, 'max_depth': 37, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "50 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.82488631 0.83242887 0.82488631 0.8280068  0.82488631 0.8278815\n",
      " 0.82488631 0.84158705 0.82488631 0.82488631 0.83346137 0.8317569\n",
      " 0.82514459 0.84935822 0.84159336 0.82488631 0.82488631 0.84062528\n",
      " 0.83856831 0.82768187 0.84863143 0.84491204 0.84276783 0.82498315\n",
      " 0.8325115  0.8278815  0.83987605 0.82488631 0.84823064 0.84542834\n",
      " 0.82627955 0.83581309 0.82790575 0.82488631 0.8418381  0.83393342\n",
      " 0.82488631 0.77844887 0.82514459 0.82488631 0.82488631 0.82586846\n",
      " 0.77844887 0.83917863 0.82487201 0.82488631 0.83628424        nan\n",
      " 0.83581309 0.8280068  0.83825509 0.8246564  0.82488631 0.8278815\n",
      " 0.83581309 0.82507291 0.83861123 0.82507291 0.84287178 0.83834604\n",
      " 0.84321027 0.83026191 0.83304336 0.8325115  0.83942426 0.82488631\n",
      " 0.82514459 0.84815873 0.83133873 0.82514459 0.82488631 0.83863819\n",
      " 0.83585906 0.82488631 0.83301534 0.82488631 0.82488631 0.83393342\n",
      " 0.82768187 0.82514459 0.83581309 0.82498315 0.84523013 0.84896625\n",
      " 0.82866581 0.82488631 0.83711882 0.77844887 0.8317697  0.8418381\n",
      "        nan 0.82855513        nan 0.77844887 0.83118427 0.82488631\n",
      " 0.82768187 0.82488631 0.82488631 0.838786   0.82488631 0.83433522\n",
      " 0.8424054  0.83762658 0.82488631 0.84708677 0.82488631 0.82857656\n",
      " 0.82488631 0.83856831 0.83498151 0.8317569  0.83609512 0.82488631\n",
      " 0.82901999 0.82488631 0.82409699 0.82488631 0.82586846 0.82488631\n",
      " 0.82627955 0.84515481 0.8280068  0.82488631 0.82488631 0.8389098\n",
      " 0.83556602 0.82488631 0.82855513 0.84282233 0.84842166 0.82488631\n",
      " 0.82488631 0.82488631 0.83212638 0.82488631 0.82488631 0.77844887\n",
      " 0.8459633  0.82875603 0.83927765 0.82488631 0.83304336 0.82602644\n",
      " 0.82447048 0.82488631 0.82488631 0.82488631 0.82567763 0.83581309\n",
      " 0.84795133 0.82856943 0.83302929 0.82488631 0.8352192  0.85295798\n",
      " 0.77844887 0.82488631        nan 0.83301534 0.83834604 0.82488631\n",
      " 0.82634772 0.83255635 0.83581309 0.8398029  0.83581309        nan\n",
      " 0.82857656 0.82488631 0.82627955 0.84499722 0.84276655 0.83783511\n",
      " 0.82488631 0.82768187 0.82627955 0.82901807 0.84501705 0.83556602\n",
      " 0.83304336 0.82817383 0.83762658 0.82488631 0.84422075 0.83332056\n",
      " 0.82488631 0.83361517 0.84428996 0.83917863 0.82488631 0.82488631\n",
      " 0.82488631 0.82857656 0.83556602 0.83241203 0.8280068  0.82487201\n",
      " 0.84224584 0.82488631 0.77844887 0.82488631 0.82891217 0.83593767\n",
      " 0.82488631 0.82488631 0.82514459 0.82488631 0.84795638 0.8507434\n",
      " 0.83146527 0.82488631 0.82464886 0.82487201 0.83800175 0.8280068\n",
      " 0.82488631 0.82488631 0.82768187 0.84116344 0.83878463 0.83480762\n",
      " 0.83304336 0.82488631 0.82575817 0.83111624 0.82487201 0.83884971\n",
      " 0.84024032 0.83480762 0.84805907 0.82488631 0.83752261 0.82488631\n",
      " 0.83494371 0.82509452 0.82488631 0.845558   0.82488631 0.84205542\n",
      " 0.82869408 0.82146644 0.82768187 0.82488631 0.84771674 0.82488631\n",
      " 0.82487201 0.82768187 0.82627955 0.83084712 0.82487201        nan\n",
      " 0.83581309 0.82488631 0.84168809 0.82488631 0.82768187 0.82039937\n",
      " 0.82571409 0.82514459 0.82488631 0.82488631 0.84221652 0.8365337\n",
      " 0.8325115  0.83997506 0.82488631 0.83742216 0.82488631 0.83304336\n",
      "        nan 0.83581309 0.82856943 0.8280068  0.84815873 0.83908633\n",
      " 0.82488631 0.82488631 0.82488631 0.83585906 0.82375908 0.84686788\n",
      " 0.82566132 0.83548537 0.82488631 0.82614606 0.82488631 0.82488631\n",
      " 0.83581309 0.82514459 0.82488631 0.84062648 0.82940688 0.82488631\n",
      " 0.82488631 0.82768187 0.82735063 0.82488631 0.82488631 0.84919681\n",
      " 0.84708677 0.77844887 0.82487201 0.82488631 0.77844887 0.82488631\n",
      " 0.83361517 0.84926223 0.83834604 0.82768187 0.84815873 0.83727502\n",
      " 0.84276655 0.82488631 0.8404274  0.84428996 0.84437875 0.84224789\n",
      " 0.8245754  0.83494371 0.83845651 0.82003648 0.83581309 0.82488631\n",
      " 0.84543013 0.77844887 0.84221652 0.84673915 0.83587396 0.82488631\n",
      " 0.82488631 0.82488631 0.83480762 0.83024761 0.82488631 0.83079495\n",
      " 0.82488631 0.8280068  0.83556602 0.84190699 0.83893887 0.84276655\n",
      " 0.82488631 0.82488631 0.84250571 0.82901999 0.83556602 0.83556602\n",
      " 0.83585906 0.83631863 0.84221652 0.84328803 0.77844887 0.82488631\n",
      " 0.82488631 0.83942426 0.82488631 0.82488631 0.8340782  0.83313707\n",
      " 0.82488631 0.84062648 0.82488631 0.83927765 0.82488631 0.83585906\n",
      " 0.77844887 0.83304336 0.82488631 0.83270994 0.83556602 0.83585906\n",
      " 0.83870358 0.82488631 0.82904928 0.84491204 0.82627955 0.8278815\n",
      " 0.82488631 0.85276431 0.82488631 0.84970515 0.82488631 0.8450845\n",
      " 0.82488631 0.84276783 0.84381077 0.82488631 0.82488631 0.84593807\n",
      " 0.82926049 0.82857656 0.82895846        nan 0.8463017  0.83302929\n",
      " 0.84708677 0.8317569  0.84649426 0.84185529 0.83242887        nan\n",
      " 0.82488631 0.85439252 0.83099226 0.82488631 0.83585906 0.82068265\n",
      " 0.82488631 0.83589091 0.82490421 0.82488631 0.83304336 0.83895825\n",
      " 0.83581309 0.84004255 0.83304336        nan 0.84315104 0.77844887\n",
      " 0.83825509 0.82249357 0.82488631 0.82940688 0.83753069 0.83783511\n",
      " 0.85236155 0.8360193  0.85140205 0.83793687 0.84649426 0.82488631\n",
      " 0.82514459 0.83231997 0.82514459 0.8352192  0.82488631 0.83026191\n",
      " 0.82586846 0.82634772 0.83997506 0.84062648 0.82627955 0.84649426\n",
      " 0.83948902 0.82488631 0.84024032 0.83956    0.83585906 0.83325434\n",
      " 0.83238518 0.83581309 0.82514459 0.82488631 0.82926049 0.77844887\n",
      " 0.83566579 0.82488631 0.83119435 0.82488631 0.82857656 0.84593807\n",
      " 0.837113   0.83370228 0.8314668  0.83304336 0.84111477 0.82507291\n",
      " 0.82488631 0.85179469 0.84307665 0.83762658 0.82488631 0.83857484\n",
      " 0.82514459 0.84428996 0.82586846 0.83893887 0.82575817 0.77844887\n",
      " 0.82514459 0.82488631 0.82586846 0.83944018 0.82488631 0.82627955\n",
      " 0.82488631 0.82488631 0.83893887 0.82487201 0.83861123 0.84975562\n",
      " 0.82488631 0.82856943 0.8280068  0.84276655 0.8280068  0.8387688\n",
      " 0.83834664 0.83133873]\n",
      "  warnings.warn(\n",
      "C:\\Users\\akhil\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.82583601 0.85252173 0.82583601 0.83272552 0.82583601 0.83301112\n",
      " 0.82583601 0.85519949 0.82583601 0.82583601 0.89053855 0.83864725\n",
      " 0.82851932 0.87079717 0.86338119 0.82583601 0.82583601 0.86575024\n",
      " 0.8437397  0.83955617 0.87382901 0.85280786 0.85487675 0.82808965\n",
      " 0.83800353 0.83301112 0.84373893 0.82583601 0.86356014 0.86092705\n",
      " 0.83082791 0.84238417 0.85645464 0.82583601 0.84834074 0.85667189\n",
      " 0.82583601 0.77774902 0.82851932 0.82583601 0.82583601 0.83087084\n",
      " 0.77774902 0.8633291  0.82811686 0.82583601 0.88728468        nan\n",
      " 0.84238417 0.83272552 0.84786933 0.86004097 0.82583601 0.83301112\n",
      " 0.84238417 0.83803457 0.84625332 0.83803457 0.85547508 0.84634707\n",
      " 0.88706104 0.83278796 0.84049152 0.83800353 0.85623245 0.82583601\n",
      " 0.82851932 0.86387474 0.83647977 0.82851932 0.82583601 0.85634039\n",
      " 0.84341642 0.82583601 0.83765142 0.82583601 0.82583601 0.85667189\n",
      " 0.83952466 0.82851932 0.84238417 0.82808965 0.85216545 0.86807203\n",
      " 0.84935776 0.82583601 0.84613639 0.77774902 0.84972497 0.84834074\n",
      "        nan 0.83219021        nan 0.77774902 0.8367705  0.82583601\n",
      " 0.83952466 0.82583601 0.82583601 0.86221127 0.82583601 0.83790805\n",
      " 0.87766082 0.84496355 0.82583601 0.85516646 0.82583601 0.84022157\n",
      " 0.82583601 0.8437397  0.85834674 0.83864725 0.84527682 0.82583601\n",
      " 0.83445672 0.82583601 0.87341988 0.82583601 0.83030851 0.82583601\n",
      " 0.83082791 0.88104095 0.83272552 0.82583601 0.82583601 0.84840174\n",
      " 0.8428255  0.82583601 0.83219021 0.85191998 0.86894943 0.82583601\n",
      " 0.82583601 0.82583601 0.84917181 0.82583601 0.82583601 0.77774902\n",
      " 0.85431476 0.84177388 0.85612207 0.82583601 0.84049152 0.9344062\n",
      " 0.85440082 0.82583601 0.82583601 0.82583601 0.83068731 0.84238417\n",
      " 0.87810671 0.82957533 0.83941205 0.82583601 0.84244045 0.86703641\n",
      " 0.77774902 0.82583601        nan 0.83765142 0.84634707 0.82583601\n",
      " 0.84140421 0.83776219 0.84238417 0.86284149 0.84238417        nan\n",
      " 0.84022157 0.82583601 0.83082791 0.85228415 0.84810619 0.85602064\n",
      " 0.82583601 0.83952466 0.83082791 0.8488114  0.85475218 0.8428255\n",
      " 0.84049152 0.8480048  0.84496355 0.82583601 0.85141261 0.83957959\n",
      " 0.82583601 0.84394939 0.84936169 0.8633291  0.82583601 0.82583601\n",
      " 0.82583601 0.84022157 0.8428255  0.86683654 0.83272552 0.82814917\n",
      " 0.85416438 0.82583601 0.77774902 0.82583601 0.83448741 0.85778507\n",
      " 0.82583601 0.82583601 0.82851932 0.82583601 0.8637977  0.86670695\n",
      " 0.83837131 0.82583601 0.91571321 0.82814917 0.86013511 0.83272552\n",
      " 0.82583601 0.82583601 0.83952466 0.86430707 0.86372918 0.84214072\n",
      " 0.84049152 0.82583601 0.83018155 0.85412487 0.82814917 0.84933135\n",
      " 0.86506767 0.84214072 0.85843965 0.82583601 0.86749405 0.82583601\n",
      " 0.84140665 0.84688781 0.82583601 0.85316171 0.82583601 0.87317776\n",
      " 0.83460667 0.83329373 0.83955617 0.82583601 0.87175973 0.82583601\n",
      " 0.82814917 0.83952466 0.83082791 0.90270969 0.82814917        nan\n",
      " 0.84238417 0.82583601 0.87416201 0.82583601 0.83955617 0.83620896\n",
      " 0.85830839 0.82848866 0.82583601 0.82583601 0.84744981 0.83996952\n",
      " 0.83800353 0.84808153 0.82583601 0.86525743 0.82583601 0.84049152\n",
      "        nan 0.84238417 0.82957533 0.83272552 0.86361976 0.86265887\n",
      " 0.82583601 0.82583601 0.82583601 0.84341642 0.89239724 0.86138449\n",
      " 0.82792791 0.84597667 0.82583601 0.82855599 0.82583601 0.82583601\n",
      " 0.84238417 0.82851932 0.82583601 0.84469235 0.83451486 0.82583601\n",
      " 0.82583601 0.83952466 0.87019712 0.82583601 0.82583601 0.86287213\n",
      " 0.85516646 0.77774902 0.82814917 0.82583601 0.77774902 0.82583601\n",
      " 0.84394939 0.8678224  0.84634707 0.83955617 0.86361976 0.84616532\n",
      " 0.84810619 0.82583601 0.86277294 0.84936169 0.85100697 0.8816056\n",
      " 0.86872792 0.84140665 0.84714625 0.91972065 0.84238417 0.82583601\n",
      " 0.86407617 0.77774902 0.84744981 0.85676346 0.8442149  0.82583601\n",
      " 0.82583601 0.82583601 0.84214072 0.83506881 0.82583601 0.83889216\n",
      " 0.82583601 0.83272552 0.8428255  0.86631783 0.84532065 0.84810619\n",
      " 0.82583601 0.82583601 0.84770077 0.83445672 0.8428255  0.8428255\n",
      " 0.84341642 0.93416042 0.84752902 0.85342315 0.77774902 0.82583601\n",
      " 0.82583601 0.85623245 0.82583601 0.82583601 0.83858438 0.88178813\n",
      " 0.82583601 0.84469235 0.82583601 0.85631015 0.82583601 0.84341642\n",
      " 0.77774902 0.84049152 0.82583601 0.86295287 0.8428255  0.84341642\n",
      " 0.84675144 0.82583601 0.83272057 0.85280786 0.83082791 0.83301112\n",
      " 0.82583601 0.87247884 0.82583601 0.85984595 0.82583601 0.85286541\n",
      " 0.82583601 0.85471897 0.85694604 0.82583601 0.82583601 0.85287561\n",
      " 0.83397202 0.84022157 0.84321744        nan 0.87603656 0.83954919\n",
      " 0.85516646 0.83864725 0.85515207 0.85258027 0.85252173        nan\n",
      " 0.82583601 0.87569253 0.85112061 0.82583601 0.84341642 0.85913823\n",
      " 0.82583601 0.85435129 0.84761819 0.82583601 0.84049152 0.86278963\n",
      " 0.84238417 0.84923917 0.84049152        nan 0.85005939 0.77774902\n",
      " 0.84786933 0.8752371  0.82583601 0.83445144 0.86718467 0.85602064\n",
      " 0.87128317 0.84408099 0.86639344 0.86166173 0.85498969 0.82583601\n",
      " 0.82848866 0.8394209  0.82848866 0.84244045 0.82583601 0.83278796\n",
      " 0.83080794 0.84140421 0.84827272 0.84469235 0.83082791 0.85498969\n",
      " 0.86968552 0.82583601 0.86506767 0.85571576 0.84341642 0.86003515\n",
      " 0.8404174  0.84238417 0.82851932 0.82583601 0.83397202 0.77774902\n",
      " 0.90664438 0.82583601 0.83905091 0.82583601 0.84022157 0.85287561\n",
      " 0.86187346 0.85928967 0.84835467 0.84049152 0.85117133 0.83803457\n",
      " 0.82583601 0.89034678 0.85163654 0.84496355 0.82583601 0.86566422\n",
      " 0.82851932 0.84936169 0.83080794 0.84532065 0.83018155 0.77774902\n",
      " 0.82851932 0.82583601 0.83080794 0.86324952 0.82583601 0.83082791\n",
      " 0.82583601 0.82583601 0.84532065 0.82814917 0.84625332 0.86445615\n",
      " 0.82583601 0.82973987 0.83272552 0.84810619 0.83272552 0.85682798\n",
      " 0.85486377 0.83647977]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,50),  \n",
    "    'min_samples_leaf': np.arange(1,50),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,60), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc387c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8462980 Precision=0.8379374 Recall=0.8568738 F1=0.8472998\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f226e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conduct an exhaustive search across a smaller range of parameters around the parameters found in the initial random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea339ca",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2549401b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "The best precision score is 0.8557167358082418\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 35, 'max_leaf_nodes': 54, 'min_impurity_decrease': 0.0031999999999999993, 'min_samples_leaf': 10, 'min_samples_split': 22}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(22,26),  \n",
    "    'min_samples_leaf': np.arange(9,13),\n",
    "    'min_impurity_decrease': np.arange(0.0029, 0.0033, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(54,58), \n",
    "    'max_depth': np.arange(35,40), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d00d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8472352 Precision=0.8552124 Recall=0.8342750 F1=0.8446139\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c867e0",
   "metadata": {},
   "source": [
    "## discussion section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we did Random search with the number of iterations of 500 and then doing Grid search around the values which we found on random serach.\n",
    "#With these values we found around 8000 fits in Grid search\n",
    "#By Comparing with these models, We found SVM (with poly kernel) has the better precision of 0.855839"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
